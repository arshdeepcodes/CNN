# -*- coding: utf-8 -*-
"""Try1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QLaZwza7Vf2WnfkvvkyDJJpXl_62_Sb7
"""

#importing basic libraries
import numpy as np
import pandas as pd

# Commented out IPython magic to ensure Python compatibility.
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline
!pip install Livelossplot
from livelossplot import PlotLossesKeras

#Mounting Drive

from google.colab import drive
drive.mount('/content/drive')

#check columns names
info=pd.read_csv('/content/drive/MyDrive/Dataset/Info.txt', sep=" ")
info=info.drop('Unnamed: 7',axis=1)
info.SEVERITY.fillna(0)
info.head(10)

info.axes

# Commented out IPython magic to ensure Python compatibility.
#importing all libraries
import numpy as np
import pandas as pd
import cv2
from PIL import Image
import scipy

import tensorflow as tf
from tensorflow.keras.applications import *
from tensorflow.keras.optimizers import *
from tensorflow.keras.losses import *
from tensorflow.keras.layers import *
from tensorflow.keras.models import *
from tensorflow.keras.callbacks import *
from tensorflow.keras.preprocessing.image import *
from tensorflow.keras.utils import *
# import pydot

from sklearn.metrics import *
from sklearn.model_selection import *
import tensorflow.keras.backend as K

from tqdm import tqdm, tqdm_notebook
#from colorama import Fore
import json
import matplotlib.pyplot as plt
import seaborn as sns
from glob import glob
from skimage.io import *
# %config Completer.use_jedi = False
import time
from sklearn.decomposition import PCA
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import lightgbm as lgb
import xgboost as xgb

print("All modules have been imported")

#comparing various features

sns.set_style('darkgrid')
fig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,5))
sns.barplot(x=info.BG.unique() , y=info.BG.value_counts() , palette='Blues_r',ax=ax1)
sns.barplot(x=info.CLASS.unique() , y=info.CLASS.value_counts() , palette='Blues_r',ax=ax2)

#Plotting images
from PIL import Image
import glob # used to search similar images
x= []
for filename in sorted(glob.glob("/content/drive/MyDrive/Dataset/all-mias/*.pgm")):
    img=cv2.imread(filename)  #to read file
    img =cv2.resize(img,(224, 224))
    x.append(img)
fig=plt.figure(figsize=(15,15))
columns = 3
rows = 3
for i in range(1, columns*rows +1):
    img = np.random.randint(10)
    fig.add_subplot(rows, columns, i)
    plt.imshow(x[i])
plt.show()

"""Defining some helper functions"""

# Image Augmentation
no_angles = 360
url = '/content/drive/MyDrive/Dataset/all-mias/'

def save_dictionary(path,data):
        print('saving catalog...')
        #open('u.item', encoding="utf-8")
        import json
        with open(path,'w') as outfile:
            json.dump(str(data), fp=outfile)
        # save to file:
        print(' catalog saved')

#train_test_split_datagen=ImageDataGenerator("augmentations such as flip,brightness range,etc....")
#val_datagen=ImageDataGenerator("augmentations such as flip,brightness range,etc....")
#test_datagen=ImageDataGenerator("augmentations such as flip,brightness range,etc....")
def read_image():
        url='/content/drive/MyDrive/Dataset/all-mias/'
        print("Reading images")
        import cv2
        info = {}
        for i in range(322):
            if i<9:
                image_name='mdb00'+str(i+1)
            elif i<99:
                image_name='mdb0'+str(i+1)
            else:
                image_name = 'mdb' + str(i+1)
            image_address= url+image_name+'.pgm'
            img = cv2.imread(image_address,1)
            #try:
            img = cv2.resize(img, (224,224))
            #except:
                #break
            rows, cols,channel = img.shape
            info[image_name]={}
            for angle in range(0,no_angles,8):
                M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)
                img_rotated = cv2.warpAffine(img, M, (cols, rows))
                info[image_name][angle]=img_rotated
        return (info)

def read_label():
    print("Reading labels")
    filename = '/content/drive/MyDrive/Dataset/Info.txt'
    text_all = open(filename).read()
    #print(text_all)
    lines=text_all.split('\n')
    info={}
    for line in lines:
        words=line.split(' ')
        if len(words)>3:
            if (words[3] == 'B'):
                info[words[0]] = {}
                for angle in range(0,no_angles,8):
                    info[words[0]][angle] = 0
            if (words[3] == 'M'):
                info[words[0]] = {}
                for  angle in range(0,no_angles,8):
                    info[words[0]][angle] = 1
    return (info)

"""Splitting training and testing data"""

import numpy as np
label_info=read_label()
image_info=read_image()
ids=label_info.keys()
#del lable_info['Truth-Data:']
X=[]
Y=[]
for id in ids:
    for angle in range(0,no_angles,8):
        X.append(str(image_info[id][angle]))
        Y.append(label_info[id][angle])
X=np.array(X)
Y=np.array(Y)
Y=to_categorical(Y,2)

#print(X.shape)
#print(Y.shape)
x_train, x_test1, y_train, y_test1 = train_test_split(X, Y, test_size=0.3, random_state=42)
x_val, x_test, y_val, y_test = train_test_split(x_test1, y_test1, test_size=0.3, random_state=42)
print(len(x_train),len(x_val),len(x_test))

"""MODEL 1:
ResNet50
"""

from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
import numpy as np
label_info=read_label()
image_info=read_image()
#print(image_info[1][0])
ids=label_info.keys()   #ids = acceptable labeled ids
#print(type(ids))
#del label_info['Truth-Data:']
#print(lable_info)
#print(ids)
X=[]
Y=[]
for id in ids:
    for angle in range(0,no_angles,8):
        X.append(image_info[id][angle])
        Y.append(label_info[id][angle])
X=np.array(X)
Y=np.array(Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=2021,shuffle=True)

rows, cols,color = x_train[0].shape
print(x_train[0].shape)

base_model = ResNet50(input_shape=(224,224,3), weights='imagenet', include_top=False)
model=Sequential()
model.add(base_model)
model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(BatchNormalization())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(BatchNormalization())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.5))
model.add(BatchNormalization())
model.add(Dense(1, activation='sigmoid'))

for layer in base_model.layers:
  layer.trainable = False

model.summary()

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
es = EarlyStopping(monitor='val_loss', mode='min', patience=4,restore_best_weights=True, verbose=1)

model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(x_train, y_train,validation_split=0.15,shuffle=True, epochs=20, batch_size=32,callbacks=[es])
loss_value , accuracy = model.evaluate(x_test, y_test)

print('Test_loss_value = ' +str(loss_value))
print('test_accuracy = ' + str(accuracy))

#print(model.predict(x_test))
#model.save('breast_cance_model.h5')

save_dictionary('history1.dat', history.history)

#%% PLOTTING RESULTS (Train vs Validation)
import matplotlib.pyplot as plt
def Train_Val_Plot(acc,val_acc,loss,val_loss):

    fig, (ax1, ax2) = plt.subplots(1,2, figsize= (15,10))
    fig.suptitle(" MODEL'S METRICS VISUALIZATION ")

    ax1.plot(range(1, len(acc) + 1), acc)
    ax1.plot(range(1, len(val_acc) + 1), val_acc)
    ax1.set_title('History of Accuracy')
    ax1.set_xlabel('Epochs')
    ax1.set_ylabel('Accuracy')
    ax1.legend(['training', 'validation'])


    ax2.plot(range(1, len(loss) + 1), loss)
    ax2.plot(range(1, len(val_loss) + 1), val_loss)
    ax2.set_title('History of Loss')
    ax2.set_xlabel('Epochs')
    ax2.set_ylabel('Loss')
    ax2.legend(['training', 'validation'])
    plt.show()


Train_Val_Plot(history.history['accuracy'],history.history['val_accuracy'],
               history.history['loss'],history.history['val_loss'])

y_pred=model.predict(x_test)
#y_pred_prb=model.predict_proba(x_test)

target=["B","M"]
from sklearn import metrics
#accuracy_score(y_true, y_pred.round(), normalize=False)
print('Accuracy:', np.round(metrics.accuracy_score(y_test, y_pred.round(),),4))
print('Precision:', np.round(metrics.precision_score(y_test, y_pred.round(), average='weighted'),4))
print('Recall:', np.round(metrics.recall_score(y_test,y_pred.round(), average='weighted'),4))
print('F1 Score:', np.round(metrics.f1_score(y_test, y_pred.round(), average='weighted'),4))
#print('ROC AUC Score:', np.round(metrics.roc_auc_score(y_test, y_pred_prb,multi_class='ovo', average='weighted'),4))
#print('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test, y_pred),4))
print('\t\tClassification Report:\n', metrics.classification_report(y_test, y_pred.round(),target_names=target))

a1=np.round(metrics.accuracy_score(y_test, y_pred.round(),),4)
p1=np.round(metrics.precision_score(y_test, y_pred.round(), average='weighted'),4)
r1=np.round(metrics.recall_score(y_test,y_pred.round(), average='weighted'),4)
f1= np.round(metrics.f1_score(y_test, y_pred.round(), average='weighted'),4)

a1,p1,r1,f1= 0.9137 ,0.9143 ,0.9137 ,0.9135

"""### MODEL 2:
VGG-16
"""

def read_label():
    print("Reading labels")
    filename = url+'Info.txt'
    text_all = open(filename).read()
    #print(text_all)
    lines=text_all.split('\n')
    info={}
    for line in lines:
        words=line.split(' ')
        if len(words)>3:
            if (words[3] == 'B'):
                info[words[0]] = {}
                for angle in range(0,no_angles,8):
                    info[words[0]][angle] = 0
            if (words[3] == 'M'):
                info[words[0]] = {}
                for  angle in range(0,no_angles,8):
                    info[words[0]][angle] = 1
        if len(words)>2:
            if (words[2] == 'NORM'):
                info[words[0]] = {}
                for angle in range(0,no_angles,8):
                    info[words[0]][angle] = 2

    return (info)

from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
import numpy as np
label_info=read_label()
image_info=read_image()
#print(image_info[1][0])
ids=label_info.keys()   #ids = acceptable labeled ids
#print(type(ids))
del label_info['Truth-Data:']
#print(lable_info)
#print(ids)
X=[]
Y=[]
for id in ids:
    for angle in range(0,no_angles,8):
        X.append(image_info[id][angle])
        Y.append(label_info[id][angle])
X=np.array(X)
Y=np.array(Y)
Y=to_categorical(Y,3)
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=2021,shuffle=True)

#%% BALANCING THE DATA DURING TRAIN
from sklearn.utils import compute_class_weight
from sklearn.utils.class_weight import compute_class_weight
y_integers = np.argmax(y_train, axis=1)
#class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)
class_weights = compute_class_weight(class_weight = "balanced", classes= np.unique(y_integers), y= y_integers)
d_class_weights = dict(enumerate(class_weights))
print(d_class_weights)

import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Conv2D, MaxPool2D, Flatten
from keras import optimizers
from keras import losses
from sklearn import metrics

rows, cols,color = x_train[0].shape
print(x_train[0].shape)

base_model = VGG16(input_shape=(224,224,3), weights='imagenet', include_top=False)
model=Sequential()
model.add(base_model)
model.add(Dropout(0.5))
model.add(Flatten())
model.add(BatchNormalization())
model.add(Dense(128,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(3,activation='softmax'))

for layer in base_model.layers:
    layer.trainable = False

model.summary()

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
#es = EarlyStopping(monitor='val_accuracy', mode='max', patience=10,restore_best_weights=True, verbose=1)
es = EarlyStopping(monitor='val_loss', mode='min', patience=6,restore_best_weights=True, verbose=1)

model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(x_train, y_train,validation_split=0.15,shuffle=True, epochs=9, batch_size=32,callbacks=[es])
loss_value , accuracy = model.evaluate(x_test, y_test)

print('Test_loss_value = ' +str(loss_value))
print('test_accuracy = ' + str(accuracy))

#print(model.predict(x_test))
#model.save('breast_cance_model.h5')

save_dictionary('history1.dat', history.history)

#%% PLOTTING RESULTS (Train vs Validation)
import matplotlib.pyplot as plt
def Train_Val_Plot(acc,val_acc,loss,val_loss):

    fig, (ax1, ax2) = plt.subplots(1,2, figsize= (15,10))
    fig.suptitle(" MODEL'S METRICS VISUALIZATION ")

    ax1.plot(range(1, len(acc) + 1), acc)
    ax1.plot(range(1, len(val_acc) + 1), val_acc)
    ax1.set_title('History of Accuracy')
    ax1.set_xlabel('Epochs')
    ax1.set_ylabel('Accuracy')
    ax1.legend(['training', 'validation'])


    ax2.plot(range(1, len(loss) + 1), loss)
    ax2.plot(range(1, len(val_loss) + 1), val_loss)
    ax2.set_title('History of Loss')
    ax2.set_xlabel('Epochs')
    ax2.set_ylabel('Loss')
    ax2.legend(['training', 'validation'])
    plt.show()


Train_Val_Plot(history.history['accuracy'],history.history['val_accuracy'],
               history.history['loss'],history.history['val_loss'])

y_pred=model.predict(x_test)
y_test=[np.argmax(x) for x in y_test]

y_test_arg=np.argmax(y_test,axis=0)
y_pred = np.argmax(model.predict(x_test),axis=1)
print('Confusion Matrix')
#print(confusion_matrix(y_test_arg, y_pred))

target=["B","M"]
from sklearn import metrics
#accuracy_score(y_true, y_pred.round(), normalize=False)
#print('Accuracy:', np.round(metrics.accuracy_score(y_test, y_pred.round(),),4))
print('Precision:', np.round(metrics.precision_score(y_test, y_pred, average='weighted'),4))
print('Recall:', np.round(metrics.recall_score(y_test,y_pred.round(), average='weighted'),4))
print('F1 Score:', np.round(metrics.f1_score(y_test, y_pred.round(), average='weighted'),4))
#print('ROC AUC Score:', np.round(metrics.roc_auc_score(y_test, y_pred_prb,multi_class='ovo', average='weighted'),4))
#print('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test, y_pred),4))
print('\t\tClassification Report:\n', metrics.classification_report(y_test, y_pred.round(),target_names=target))


p2=np.round(metrics.precision_score(y_test, y_pred, average='weighted'),4)
r2=np.round(metrics.recall_score(y_test,y_pred.round(), average='weighted'),4)
f2=np.round(metrics.f1_score(y_test, y_pred.round(), average='weighted'),4)

a2,p2,r2,f2= 0.9514,0.9515 ,0.9514 ,0.9516

"""### MODEL 3:
DenseNet169
"""

from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
import numpy as np
label_info=read_label()
image_info=read_image()
#print(image_info[1][0])
ids=label_info.keys()   #ids = acceptable labeled ids
#print(type(ids))
#del label_info['Truth-Data:']
#print(lable_info)
#print(ids)
X=[]
Y=[]
for id in ids:
    for angle in range(0,no_angles,8):
        X.append(image_info[id][angle])
        Y.append(label_info[id][angle])
X=np.array(X)
Y=np.array(Y)
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=2021,shuffle=True)
# cancer_prediction_cnn(x_train, y_train, x_test, y_test)

base_model = DenseNet169(input_shape=(224,224,3), weights='imagenet', include_top=False)
model=Sequential()
model.add(base_model)
model.add(Dropout(0.2))
model.add(Flatten())
model.add(BatchNormalization())
model.add(Dense(1024,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(Dense(1024,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.2))
model.add(Dense(1,activation='sigmoid'))

for layer in base_model.layers:
    layer.trainable = False

model.summary()
(224, 224, 3)

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
es = EarlyStopping(monitor='val_loss', mode='min', patience=8,restore_best_weights=True, verbose=1)

model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(x_train, y_train,validation_split=0.15,shuffle=True, epochs=10, batch_size=32,callbacks=[es])
loss_value , accuracy = model.evaluate(x_test, y_test)

print('Test_loss_value = ' +str(loss_value))
print('test_accuracy = ' + str(accuracy))

#print(model.predict(x_test))
#model.save('breast_cance_model.h5')

save_dictionary('history1.dat', history.history)

#%% PLOTTING RESULTS (Train vs Validation)
import matplotlib.pyplot as plt
def Train_Val_Plot(acc,val_acc,loss,val_loss):

    fig, (ax1, ax2) = plt.subplots(1,2, figsize= (15,10))
    fig.suptitle(" MODEL'S METRICS VISUALIZATION ")

    ax1.plot(range(1, len(acc) + 1), acc)
    ax1.plot(range(1, len(val_acc) + 1), val_acc)
    ax1.set_title('History of Accuracy')
    ax1.set_xlabel('Epochs')
    ax1.set_ylabel('Accuracy')
    ax1.legend(['training', 'validation'])


    ax2.plot(range(1, len(loss) + 1), loss)
    ax2.plot(range(1, len(val_loss) + 1), val_loss)
    ax2.set_title('History of Loss')
    ax2.set_xlabel('Epochs')
    ax2.set_ylabel('Loss')
    ax2.legend(['training', 'validation'])
    plt.show()


Train_Val_Plot(history.history['accuracy'],history.history['val_accuracy'],
               history.history['loss'],history.history['val_loss'])

y_pred=model.predict(x_test)

target=["B","M"]
from sklearn import metrics
#accuracy_score(y_true, y_pred.round(), normalize=False)
print('Accuracy:', np.round(metrics.accuracy_score(y_test, y_pred.round(),),4))
print('Precision:', np.round(metrics.precision_score(y_test, y_pred.round(), average='weighted'),4))
print('Recall:', np.round(metrics.recall_score(y_test,y_pred.round(), average='weighted'),4))
print('F1 Score:', np.round(metrics.f1_score(y_test, y_pred.round(), average='weighted'),4))
#print('ROC AUC Score:', np.round(metrics.roc_auc_score(y_test, y_pred_prb,multi_class='ovo', average='weighted'),4))
#print('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test, y_pred),4))
print('\t\tClassification Report:\n', metrics.classification_report(y_test, y_pred.round(),target_names=target))

a3=np.round(metrics.accuracy_score(y_test, y_pred.round(),),4)
p3=np.round(metrics.precision_score(y_test, y_pred.round(), average='weighted'),4)
r3=np.round(metrics.recall_score(y_test,y_pred.round(), average='weighted'),4)
f3=np.round(metrics.f1_score(y_test, y_pred.round(), average='weighted'),4)

a3,p3,r3,f3=0.8417,0.8452,0.8417,0.8419

# Define a color palette using Seaborn
palette = sns.color_palette("husl", len(compare_results))

# Assuming you have a DataFrame called compare_results
compare_results.set_index("model_name")["model_acc"].plot(kind="barh", color=palette)

plt.xlabel("Accuracy (%)")
plt.ylabel("Model")
plt.title("Model Accuracy")
plt.show()

a=[[],[],[]]

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Model names and metric names
model_names = ['ResNet50','VGG16','DenseNet169']
metric_names = [ 'Accuracy', 'Precision','Recall', 'F1 Score']

# Metric values for each model
values = np.array([
    [a1, p1, r1, f1],
    [a2, p2, r2, f2],
    [a3,p3,r3,f3]
])

# Define bar width
bar_width = 0.2
index = np.arange(len(model_names))

# Create a grouped bar chart
fig, ax = plt.subplots(figsize=(10, 6))

# Use Seaborn's "viridis" color palette for distinct colors
palette = sns.color_palette("Set2")

for i, metric_name in enumerate(metric_names):
    bars = ax.bar(index + i * bar_width, values[:, i], bar_width, label=metric_name, color=palette[i])
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.2f}',  # Format the value to 2 decimal places
                    xy=(bar.get_x() + bar.get_width() / 2, height),  # Position to annotate
                    xytext=(0, 3),  # Offset to adjust the text position
                    textcoords="offset points",
                    ha='center', va='bottom')

# Set the x-axis labels
ax.set_xticks(index + bar_width * (len(metric_names) - 1) / 2)
ax.set_xticklabels(model_names)

# Set the y-axis label
ax.set_ylabel('Metric Value')

# Set the chart title
ax.set_title('Comparison of Metrics for 3 Models')

# Adjust the legend location
ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1))

# Show the grouped bar chart
plt.show()
